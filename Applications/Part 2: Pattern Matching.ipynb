{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Matching and Text Extraction\n",
    "When we deal with vast texts, it is essential to be able to find some explicit fragments that convey the information we need. This technique is useful in *question answering* where applied linguists can design systems that will be capable to answer questions by searching the information in the text. \n",
    "# Regular expressions\n",
    "## Static expressions\n",
    "The most primitive approach to fulfil this task is to search through the pattern of characters that are supposed to identify the piece of text one needs. It's highly specific and depends a lot on the choice of words, register and style of speech, but sometimes character matching is the easiest way to find what cannot be conveyed through grammatical relations easily.  \n",
    "\n",
    "The way how regular expressions work is by specifying a special pattern that can contain hard characters, loops, gaps, sets and intervals, and the special method will return all segments from the text that correspond to the pattern. When only words are used as a regular expression, they will yield of instances of said words encountered in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 5), match='day'>\n",
      "<re.Match object; span=(16, 19), match='day'>\n",
      "<re.Match object; span=(45, 48), match='day'>\n"
     ]
    }
   ],
   "source": [
    "#Static regular expression showcase\n",
    "import re #regular expression module\n",
    "pattern = re.compile(r'day') \n",
    "text = \"Today is a good day, why not make it a great day?\"\n",
    "for match in pattern.finditer(text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that static regular expression not only returned `Match` objects that are standalone words, but also extracted the char sequence if it's a part of words.  \n",
    "\n",
    "## Dynamic expressions and quantifiers\n",
    "Regular expressions are much more powerful when used *dynamically*, that is with certain flexibility that other characters can embody. In regular expressions, developers can specify how many times some character must be repeated in the following fashion:\n",
    "* `+` matches pattern one or more times (making it obligatory);\n",
    "* `?` matches pattern one or zero times (making it optional);\n",
    "* `.` matches any character;\n",
    "* `{m, n}` matches pattern at least `m` times but not more than `n`.\n",
    "Analogically, `{,n}` matches up to n times and `{m,}` matches if the pattern repeats more than `m` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(14, 19), match='bread'>\n",
      "<re.Match object; span=(45, 50), match='brad.'>\n"
     ]
    }
   ],
   "source": [
    "# Dynamic regular expression showcase\n",
    "pattern = re.compile(\"br.{3}\")\n",
    "text = \"I like to eat bread, but I don't like to eat brad.\"\n",
    "for match in pattern.finditer(text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets and ranges\n",
    "**Sets** is a way to tell that one of the characters from a set must follow. They can be as simple as specific characters and extend to nested regular expressions. Regular expressions also support a number of *character classes* to convey a group of characters more easily, such as ASCII, alphanumeric, digits, etc.  \n",
    "\n",
    "In the event if there is no available character class, it makes sense to create your own with **ranges**. Ranges essentially allow users to create custom sets that consist from all characters that lie in a range. For example, `[a-z]` is the range that includes all lowercase Latin symbols, while `[0-9]` encompasses all numbers. Ranges tend to be suffixed with a quantifier to express how many times a single character from a set of range must repeat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='Alicia'>\n",
      "<re.Match object; span=(17, 25), match='Anaconda'>\n",
      "--------------------\n",
      "<re.Match object; span=(0, 3), match='Can'>\n",
      "<re.Match object; span=(36, 42), match='France'>\n",
      "--------------------\n",
      "<re.Match object; span=(0, 15), match='Lucky is my cat'>\n"
     ]
    }
   ],
   "source": [
    "# Sets, ranges and character classes\n",
    "pattern = re.compile(r\"A[a-z]+a\")\n",
    "text = \"Alicia installed Anaconda to get started with Python.\"\n",
    "for match in pattern.finditer(text):\n",
    "    print(match)\n",
    "print(\"--------------------\")\n",
    "pattern = re.compile(r\"[A-Z][a-z]+\") #Any word that starts with a capital letter\n",
    "text = \"Can you tell me what the capital of France is?\"\n",
    "for match in pattern.finditer(text):\n",
    "    print(match)\n",
    "print(\"--------------------\")\n",
    "pattern = re.compile(r\"Lucky is my (cat|dog)\") #Either cat or dog\n",
    "text = \"Lucky is my cat, but I wish he was my dog.\"\n",
    "for match in pattern.finditer(text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation\n",
    "Regular expressions also allow people to match everything except for some symbols they wish to be ignored. They are declared within `[^a]` where `a` are the characters to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(7, 10), match='hat'>\n",
      "<re.Match object; span=(30, 33), match='cat'>\n"
     ]
    }
   ],
   "source": [
    "# Ignorance showcase\n",
    "pattern = re.compile(r\"[^b]at\")#Any three character word that contains -at, but does not start with b\n",
    "text = \"I like hats and bats, but not cats.\"\n",
    "for match in pattern.finditer(text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are great when we can hook on the characters and rely on punctuation, however in linguistics we often care more about the syntactic relationships between the words. For this case, the Spacy library offers us a way to match patterns by a diversity of tags that convey parts of speech, function in the sentence and named entities..\n",
    "\n",
    "# spaCy patterns\n",
    "## Hard matching\n",
    "Spacy matcher works by specifying the pattern as a list of dictionaries where each dictionary is responsible for a single requirement and assigns the a value to a number of properties. It allows linguists to match the patterns in a similar way how we discussed in regex at the beginning revolving around hard-coded words. The property `ORTH` matches all exact occurrences in the text and can be paired with `LENGTH` to specify the amount of characters. `LOWER` will also match all case-insensitive encounters with a given word in the lowercase (such as LOWER \"master\" will match \"MasTer\", \"maStER\", \"MASTer\", etc.) and `LEMMA` matches all words whose lemma corresponds to the specified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12921484011366634418, 0, 4)]\n",
      "Rabbits are mammals in\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\"Rabbits are mammals in the family Leporidae of the order Lagomorpha.\")\n",
    "pattern = [{\"LEMMA\": \"rabbit\"}, {\"LEMMA\": \"be\"}, {\"LOWER\": \"mammals\"}, {\"ORTH\": \"in\"}]\n",
    "#This pattern will match any derivative of the word \"rabbit\" followed by the word \"be\", \"mammals\" and \"in\"\n",
    "matcher.add(\"RABBIT\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "for id, start, end in matches:\n",
    "    print( doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there we see the Spacy matcher workflow: first we need to import `Matcher` and language model, describe the pattern as a list of dictionaries and the `Doc` object that contains the text, after which we can add the pattern to the matcher and extract the matches by passing the doc to it. It's notable possible to initialise multiple patterns that will be mixed together, and giving a name for them also allows to extract the pattern rules later.  \n",
    "\n",
    "## Part of speech tagging\n",
    "It is helpful to extend the matching capabilities to encompass parts of speech. The `POS` and `TAG` properties allow us to match by parts of speech and linguistic tags that convey the function in sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 13:42:55.586114: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-30 13:42:55.734408: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-30 13:42:55.734450: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-30 13:42:55.766687: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-30 13:42:56.524052: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-30 13:42:56.524135: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-30 13:42:56.524145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-10-30 13:42:57.704094: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-30 13:42:57.704137: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-30 13:42:57.704160: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-9459af): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8526073191477206738, 5, 7)]\n",
      "North America\n"
     ]
    }
   ],
   "source": [
    "text = \"Canada is a country in North America. It is the second largest country in the world.\"\n",
    "doc = nlp(text)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}] #Matches two proper nouns\n",
    "matcher.add(\"GPE_START\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "for id, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy represents the patterns as a list of dictionaries where each dictionary ressambles one lexeme. In our `pattern` variable, `{\"POS\": \"PROPN\"}` says the first word must be a proper noun, and the second word must be the same, which returned us \"North America\". Then the matches are just a list of tuples where the first value is a long integer that indexes the word in the `nlp.vocab` list, and the other two are the indexes of the match in the doc container (not characters!)  \n",
    "\n",
    "## Boolean matches\n",
    "What makes Spacy matcher must more powerful than regex is the fact that it analyses the sentence structure and we as developers can hook on a verity of its flags, which include: url, email, sentence start, number; and it also offers common analysis we do in regex too: is lower, upper, title, whitespace, stopword, punctuation character, ASCII, digit, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14506779758367473159, 0, 3), (2582013287274679728, 10, 11)]\n",
      "10 million Ukrainians\n",
      "washingtonpost.com\n"
     ]
    }
   ],
   "source": [
    "text = \"10 million Ukrainians were killed in the Holodomor reported on washingtonpost.com.\"\n",
    "doc = nlp(text)\n",
    "#Matches a digit, followed by the word million, followed by the word Ukrainians\n",
    "fact = [{\"IS_DIGIT\": True}, {\"LOWER\": \"million\"}, {\"ORTH\": \"Ukrainians\"}]\n",
    "url = [{\"LIKE_URL\": True}] #Matches a URL\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"URL\", [url])\n",
    "matcher.add(\"FACT\", [fact])\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "for id, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic and morphological matching\n",
    "Since Spacy `Doc` class parses the sentence tree, allowing us to specify grammar properties of tokens to search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity recognition\n",
    "Finally, we can also hook on the entities Doc recognises to search for some specific entities. Spacy does especially great there as we can create our own custom pipes to edit the named entity recognition API for a particular doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America\n",
      "Ukraine\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import filter_spans\n",
    "doc = nlp(\"The United States of America is the leader in military aid for Ukraine.\")\n",
    "pattern = [{\"ENT_TYPE\": \"GPE\", \"OP\": \"*\"}] #Matches a GPE followed by a proper noun\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"GPE_PROPN\", [pattern])\n",
    "matches = matcher(doc, as_spans=True)\n",
    "#print(matches)\n",
    "for match in filter_spans(matches): \n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using with named entities, Spacy tends to output one entity in a number of smaller overlapping pieces of text. To solve this issue, `filter_spans()` function was designed defined in `spacy.util` module.  \n",
    "\n",
    "# Conclusions\n",
    "Regular expressions and Spacy Matcher are two powerful matching algorithms that allow data scientists to search for text fragments with robust capabilities in both character-bound manner and transcending into higher level linguistic details. Together they cover nearly any usecases when matching is needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
